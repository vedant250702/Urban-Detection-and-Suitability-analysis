{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ae93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class MITBuildingsDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "\n",
    "        # --- Collect all image & mask paths ---\n",
    "        self.image_paths = sorted(\n",
    "            glob.glob(os.path.join(image_dir, \"*.png\")) +\n",
    "            glob.glob(os.path.join(image_dir, \"*.jpg\")) +\n",
    "            glob.glob(os.path.join(image_dir, \"*.jpeg\"))+\n",
    "            glob.glob(os.path.join(image_dir, \"*.tiff\"))\n",
    "        )\n",
    "\n",
    "        self.mask_paths = sorted(\n",
    "            glob.glob(os.path.join(mask_dir, \"*.png\")) +\n",
    "            glob.glob(os.path.join(mask_dir, \"*.jpg\")) +\n",
    "            glob.glob(os.path.join(mask_dir, \"*.jpeg\"))+\n",
    "            glob.glob(os.path.join(image_dir, \"*.tiff\"))\n",
    "        )\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        # --- Ensure equal number of files ---\n",
    "        assert len(self.image_paths) == len(self.mask_paths), \\\n",
    "            f\"Found {len(self.image_paths)} images but {len(self.mask_paths)} masks\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        # --- Load image and mask ---\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        # --- Resize to 512x512 ---\n",
    "        img = img.resize((512, 512))\n",
    "        mask = mask.resize((512, 512))\n",
    "\n",
    "        # --- Convert to tensor ---\n",
    "        img = transforms.ToTensor()(img)\n",
    "\n",
    "        mask = np.array(mask, dtype=np.float32)\n",
    "        mask = mask / 255.0  # normalize 0‚Äì1\n",
    "        mask = np.expand_dims(mask, axis=0)\n",
    "        mask = torch.tensor(mask, dtype=torch.float32)\n",
    "        mask = (mask > 0.5).float()  # binarize\n",
    "\n",
    "        # --- Apply optional transforms ---\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=img, mask=mask)\n",
    "            img = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"]\n",
    "\n",
    "        return img, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afc714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================================================\n",
    "# 1Ô∏è‚É£ U-NET MODEL\n",
    "# =========================================================\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        def conv_block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_c, out_c, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        self.enc1 = conv_block(in_channels, 64)\n",
    "        self.enc2 = conv_block(64, 128)\n",
    "        self.enc3 = conv_block(128, 256)\n",
    "        self.enc4 = conv_block(256, 512)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck = conv_block(512, 1024)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
    "        self.dec4 = conv_block(1024, 512)\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.dec3 = conv_block(512, 256)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec2 = conv_block(256, 128)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec1 = conv_block(128, 64)\n",
    "\n",
    "        self.final = nn.Conv2d(64, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.enc1(x)\n",
    "        p1 = self.pool(c1)\n",
    "\n",
    "        c2 = self.enc2(p1)\n",
    "        p2 = self.pool(c2)\n",
    "\n",
    "        c3 = self.enc3(p2)\n",
    "        p3 = self.pool(c3)\n",
    "\n",
    "        c4 = self.enc4(p3)\n",
    "        p4 = self.pool(c4)\n",
    "\n",
    "        bn = self.bottleneck(p4)\n",
    "\n",
    "        u4 = self.upconv4(bn)\n",
    "        u4 = torch.cat([u4, c4], dim=1)\n",
    "        d4 = self.dec4(u4)\n",
    "\n",
    "        u3 = self.upconv3(d4)\n",
    "        u3 = torch.cat([u3, c3], dim=1)\n",
    "        d3 = self.dec3(u3)\n",
    "\n",
    "        u2 = self.upconv2(d3)\n",
    "        u2 = torch.cat([u2, c2], dim=1)\n",
    "        d2 = self.dec2(u2)\n",
    "\n",
    "        u1 = self.upconv1(d2)\n",
    "        u1 = torch.cat([u1, c1], dim=1)\n",
    "        d1 = self.dec1(u1)\n",
    "\n",
    "        out = self.final(d1)  # no sigmoid here\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c38900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_accuracy(preds, masks):\n",
    "    \"\"\"Compute pixel-wise binary segmentation accuracy.\"\"\"\n",
    "    preds_bin = (preds > 0.5).float()\n",
    "    correct = (preds_bin == masks).float().sum()\n",
    "    total = masks.numel()\n",
    "    return (correct / total).item()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs=20, lr=1e-4, save_dir=\"models\"):\n",
    "    \"\"\"\n",
    "    Train U-Net model for binary segmentation and plot loss & accuracy curves.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Store metrics\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\nüöÄ Epoch {epoch}/{epochs}\")\n",
    "        \n",
    "        # ----- Training -----\n",
    "        model.train()\n",
    "        train_loss, train_acc = 0.0, 0.0\n",
    "\n",
    "        for imgs, masks in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_acc += calculate_accuracy(preds, masks)\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_train_acc = train_acc / len(train_loader)\n",
    "\n",
    "        # ----- Validation -----\n",
    "        model.eval()\n",
    "        val_loss, val_acc = 0.0, 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "                imgs, masks = imgs.to(device), masks.to(device)\n",
    "                preds = model(imgs)\n",
    "                loss = criterion(preds, masks)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_acc += calculate_accuracy(preds, masks)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_acc = val_acc / len(val_loader)\n",
    "\n",
    "        # Save metrics\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_acc'].append(avg_train_acc)\n",
    "        history['val_acc'].append(avg_val_acc)\n",
    "\n",
    "        print(f\"üìâ Train Loss: {avg_train_loss:.4f} | üßæ Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"‚úÖ Train Acc: {avg_train_acc*100:.2f}% | üîç Val Acc: {avg_val_acc*100:.2f}%\")\n",
    "\n",
    "        # ----- Save best model -----\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            save_path = os.path.join(save_dir, \"unet_best.pth\")\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"üíæ Best model saved at: {save_path}\")\n",
    "\n",
    "    print(\"\\nüéØ Training complete!\")\n",
    "\n",
    "    # ----- Plot Loss & Accuracy -----\n",
    "    epochs_range = range(1, epochs + 1)\n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "    # Loss\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs_range, history['train_loss'], label='Train Loss')\n",
    "    plt.plot(epochs_range, history['val_loss'], label='Val Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training & Validation Loss')\n",
    "    plt.legend()\n",
    "    # Accuracy\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs_range, [a*100 for a in history['train_acc']], label='Train Acc')\n",
    "    plt.plot(epochs_range, [a*100 for a in history['val_acc']], label='Val Acc')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training & Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "def predict_single_image(model_path, image_path, device=None, output_path=\"output_mask.png\"):\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load model\n",
    "    model = UNet(in_channels=3, out_channels=1)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Preprocess image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    # Predict mask\n",
    "    with torch.no_grad():\n",
    "        pred = model(tensor)\n",
    "        pred = torch.sigmoid(pred)  # convert logits ‚Üí probabilities\n",
    "        pred_mask = (pred.squeeze().cpu().numpy() > 0.5).astype(np.uint8) * 255\n",
    "\n",
    "    # Display input and mask\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Input Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(pred_mask, cmap=\"gray\")\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    # Save predicted mask\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    cv2.imwrite(output_path, pred_mask)\n",
    "    print(f\"‚úÖ Predicted mask saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833065e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    # --- Device setup ---\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # device=\"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- Paths ---\n",
    "    train_img_dir = \"Data/train\"\n",
    "    train_mask_dir = \"Data/train_mask\"\n",
    "    val_img_dir = \"Data/validate\"\n",
    "    val_mask_dir = \"Data/validate_mask\"\n",
    "    # model_save_path = \"models/unet_model.pth\"\n",
    "    prediction_input_dir = \"images\"               # all combined tiles\n",
    "    prediction_output_dir = \"output/predicted_masks\"\n",
    "\n",
    "    # --- Ensure model/output dirs exist ---\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    os.makedirs(prediction_output_dir, exist_ok=True)\n",
    "\n",
    "    # --- Load datasets ---\n",
    "    train_dataset = MITBuildingsDataset(train_img_dir, train_mask_dir)\n",
    "    val_dataset = MITBuildingsDataset(val_img_dir, val_mask_dir)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    # --- Initialize U-Net model ---\n",
    "    model = UNet(in_channels=3, out_channels=1)\n",
    "\n",
    "    # --- Train the model ---\n",
    "    history=train_model(model, train_loader, val_loader, device, epochs=1, lr=1e-4)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
